# AILM-benchmark

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

ä¸€ä¸ªå¼€æºçš„ç»¼åˆæ€§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºå‡†æµ‹è¯•å·¥å…·åŒ…ï¼Œæä¾›æ ‡å‡†åŒ–çš„è¯„ä¼°è„šæœ¬ã€å¯è§†åŒ–å·¥å…·å’Œæ–‡æ¡£æŒ‡å—ã€‚

## ğŸ“Œ æ ¸å¿ƒåŠŸèƒ½

- **å¤šæ¡†æ¶æ”¯æŒ**
  - å…¼å®¹ Hugging Face Transformers / OpenAI API / Anthropic ç­‰ä¸»æµæ¥å£
  - æ”¯æŒå•æœºå¤šå¡åˆ†å¸ƒå¼æ¨ç†
  
- **æ ‡å‡†åŒ–æµ‹è¯•é›†**
  - å†…ç½® 20+ å¸¸è§NLPåŸºå‡†ä»»åŠ¡ï¼ˆåŒ…æ‹¬GLUEã€SuperGLUEã€HELMç­‰ï¼‰
  - æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†å¯¼å…¥
  
- **å¤šç»´è¯„ä¼°æŒ‡æ ‡**
  - å‡†ç¡®æ€§ï¼šç²¾ç¡®ç‡/å¬å›ç‡/F1
  - æ•ˆç‡ï¼šTokens per second/å†…å­˜å ç”¨
  - æˆæœ¬ä¼°ç®—ï¼šAPIè°ƒç”¨æˆæœ¬æ¨¡æ‹Ÿå™¨
  
- **å¯è§†åŒ–æŠ¥å‘Š**
  - è‡ªåŠ¨ç”Ÿæˆäº¤äº’å¼è¯„ä¼°ä»ªè¡¨ç›˜
  - å¤šæ¨¡å‹å¯¹æ¯”é›·è¾¾å›¾
  - èµ„æºæ¶ˆè€—çƒ­åŠ›å›¾

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
git clone https://github.com/ronysun/AILM-benckmark.git
cd AILM-benchmark
pip install -r requirements.txt
```

### åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```python
from benchmark import LLMEvaluator

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = LLMEvaluator(
    model_name="gpt-3.5-turbo",
    device="cuda:0",  # æˆ– "cpu"
    precision="fp16"
)

# è¿è¡Œæ ‡å‡†æµ‹è¯•å¥—ä»¶
results = evaluator.run_standard_suite(
    tasks=["text_classification", "summarization"],
    batch_size=32,
    max_length=512
)

# ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
evaluator.generate_report(
    output_format="html",
    compare_with=["llama-2-7b", "mistral-7b"]
)
```

## ğŸ“Š é¢„ç½®æµ‹è¯•ä»»åŠ¡

| ä»»åŠ¡ç±»åˆ«       | åŒ…å«æ•°æ®é›†                     | è¯„ä¼°æŒ‡æ ‡                     |
|----------------|--------------------------------|------------------------------|
| æ–‡æœ¬åˆ†ç±»       | IMDB, AG News                  | Accuracy, F1                 |
| é—®ç­”ç³»ç»Ÿ       | SQuAD v2, HotpotQA             | EM, F1                       |
| æ–‡æœ¬ç”Ÿæˆ       | CNN/DailyMail                  | ROUGE, BLEU                  |
| æ¨ç†ä»»åŠ¡       | GSM8K, MATH                    | Accuracy                     |
| ä»£ç ç”Ÿæˆ       | HumanEval, MBPP                | Pass@k                       |

## ğŸ”§ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰æµ‹è¯•æ¨¡æ¿

```yaml
# custom_benchmark.yaml
task_config:
  name: "legal_ner"
  description: "æ³•å¾‹æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«"
  dataset:
    path: "./data/legal_ner.jsonl"
    format: "jsonlines"
  metrics:
    - name: "f1"
      args:
        average: "macro"
  constraints:
    max_input_length: 2048
    min_samples: 1000
```

### åˆ†å¸ƒå¼åŸºå‡†æµ‹è¯•
```bash
python run_distributed.py \
  --model meta-llama/Llama-2-13b-hf \
  --tasks all \
  --gpus 4 \
  --batch_sizes 16 32 64 \
  --output_dir ./results
```

## ğŸ“ˆ ç»“æœå¯è§†åŒ–ç¤ºä¾‹
![Benchmark Dashboard](docs/images/dashboard_preview.png)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼å‚ä¸è´¡çŒ®ï¼š
1. æäº¤æ–°çš„æµ‹è¯•æ•°æ®é›†
2. æ·»åŠ å¯¹æ–°æ¨¡å‹æ¶æ„çš„æ”¯æŒ
3. æ”¹è¿›è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ–¹æ³•
4. ä¼˜åŒ–æ€§èƒ½ç›‘æ§æ¨¡å—

è¯·å…ˆé˜…è¯»[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)å¹¶ç­¾ç½²è´¡çŒ®è€…åè®®ã€‚

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE)

## ğŸ“¬ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- GitHub Issues
- Email: ronysun@qq.com

---
